{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Global MetaONNX workflow\n\nThis example demonstrates how to deploy an ONNX model to Akida hardware using\nthe onnx2akida toolkit. Starting from an ONNX model, we'll show how to:\n\n1. Convert and analyze compatibility with Akida hardware\n2. Display compatibility reports\n3. Create hybrid models combining Akida-compatible and ONNX operators\n4. Generate inference models for deployment\n\nWe'll use a MobileNetV4 model exported from HuggingFace as our example, though the workflow\napplies to any ONNX model.\n\n.. figure:: ../../img/execution_flow.png\n   :target: ../../_images/execution_flow.png\n   :alt: Overall MetaONNX workflow\n   :scale: 25 %\n   :align: center\n\n   Global MetaONNX workflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Export model to ONNX format\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1. Export MobileNetV4 from HuggingFace\n\nWe'll export a MobileNetV4 model from HuggingFace using the Optimum library.\nThis demonstrates the typical workflow of obtaining an ONNX model for analysis.\n\nYou can also export models from other frameworks:\n\n* [tf2onnx](https://onnxruntime.ai/docs/tutorials/tf-get-started.html)_ for TensorFlow\n* [torch.onnx](https://docs.pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html)_ for PyTorch\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nfrom optimum.exporters.onnx import main_export\n\nmodel_dir = \"mbv4\"\nmain_export(\"timm/mobilenetv4_conv_small.e2400_r224_in1k\", output=model_dir)\nprint(f\"Model exported to {model_dir}/model.onnx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2. Load the ONNX model\n\nLoad the exported ONNX model for analysis.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import onnx\n\nmodel_path = os.path.join(model_dir, \"model.onnx\")\nmodel = onnx.load(model_path)\nprint(f\"\\nLoaded ONNX model from {model_path}\")\nprint(f\"Model has {len(model.graph.node)} nodes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Convert to Akida\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1. Convert and get compatibility information\n\nThe main entry point is the [convert](../../api_reference/onnx2akida_apis.html#onnx2akida.convert)_\nfunction, which analyzes the ONNX model and returns both a [HybridModel](../../api_reference/onnx2akida_apis.html#hybridmodel)_ and detailed compatibility information.\nThe `input_shape` parameter specifies the expected input dimensions for the model. Models can be\nexported with a dynamic shape, but quantization and later Akida conversion and mapping need all\ninput dimensions to be fixed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from onnx2akida import convert\n\n# Convert the model and analyze compatibility\n# For MobileNetV4, the input shape is (channels, height, width)\nprint(\"\\nAnalyzing model compatibility with Akida hardware...\")\nhybrid_model, compatibility_info = convert(model, input_shape=(3, 224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2. Display compatibility report\n\nThe obtained [ModelCompatibilityInfo](../../api_reference/onnx2akida_apis.html#onnx2akida.compatibility_info.ModelCompatibilityInfo)_\nobject contains detailed information about which nodes and subgraphs are compatible with Akida\nhardware. Use [print_report](../../api_reference/onnx2akida_apis.html#onnx2akida.print_report)_\nto display a comprehensive analysis.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from onnx2akida import print_report\n\n# Print detailed compatibility report\nprint_report(compatibility_info, hybrid_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The report shows:\n\n- The list of incompatibles operation types,\n- The list of incompatibilities indexed by node and by stage (quantization, conversion, mapping)\n  indicating where an incompatibility was found and why,\n- Overall compatibility percentage,\n- The memory report for Akida to CPU transfers.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3. Understanding the HybridModel\n\nThe returned [HybridModel](../../api_reference/onnx2akida_apis.html#hybridmodel)_ object\nrepresents a model that can contain both:\n\n* Akida-compatible submodels (will be accelerated on Akida hardware)\n* Standard ONNX operators (will run on CPU via ONNXRuntime)\n\nThis hybrid approach allows partial acceleration even when not all operations\nare Akida-compatible.\n\n.. Warning:: Inference is not possible on the `HybridModel` directly. You have to explicitely\n             generate an inference model as shown in the next section.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Generate inference model\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1. Generate hybrid inference model with Akida device\n\nTo create a deployable inference model, you need an Akida device.\n\n.. important::\n   A 2.0 FPGA device like available in [Akida Cloud](https://brainchip.com/aclp/)_ is used here\n   for demonstration.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import akida\n\n# Check for available Akida devices\nassert len(devices := akida.devices()) > 0, \"No device found, this example needs a 2.0 device.\"\nprint(f'Available devices: {[dev.desc for dev in devices]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inference happens on a device, so we need to map the hybrid model onto it. This can be done using\n[HybridModel.map](../../api_reference/onnx2akida_apis.html#onnx2akida.hybrid_model.HybridModel.map)_ like shown\nbelow.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Map on the device\nfpga_device = devices[0]\ntry:\n    hybrid_model.map(fpga_device)\nexcept RuntimeError as e:\n    print(\"Mapping failed:\\n\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mapping the HybridModel onto the Akida device after conversion might fail: while some layers are\nsupported by Akida hardware, they might not fit on device due to resource constraints.\nIn such cases, you can try mapping on a larger virtual device - but that cannot be used for\ninference, it only serves for prototyping - or you can go back to model conversion and provide the\ndevice as a [convert](../../api_reference/onnx2akida_apis.html#onnx2akida.convert)_ parameter.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hybrid_model, compatibility_info = convert(model, input_shape=(3, 224, 224), device=fpga_device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print_report(compatibility_info, hybrid_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The conversion algorithm knows the resource limitations, so it now avoids converting parts\nthat do not fit on the device. That is why there are more incompatibilities (the node that was too\nbig to fit on 6-node device will run on CPU), but operations that were mapped on the device can be\naccelerated by it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate the inference model\ninfer_model = hybrid_model.generate_inference_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2. Save the inference model\n\nOnce generated, the inference model can be saved for deployment.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inference_model_path = \"model_inference.onnx\"\nonnx.save(infer_model, inference_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The inference model is a standard ONNX model that can be executed using ONNXRuntime. It's graph\ncan be visualised with [Netron](https://netron.app)_ and it will show ``AkidaOp`` nodes that are\ncustom wrappers for all Akida-accelerated submodels. It will also contain ``Transpose`` nodes\nbetween ONNX and AkidaOp operators are automatically inserted to handle the different data layout\nconventions (NCHW for ONNX, NHWC for Akida).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3. Perform an inference\n\nThe inference model can be executed using ONNXRuntime and the provided [AkidaInferenceSession](../../api_reference/onnx2akida_apis.html#onnx2akida.inference.AkidaInferenceSession)_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom onnx2akida.inference import AkidaInferenceSession\n\n# Generate random input samples with shape (batch_size, channels, height, width)\ninput_samples = np.random.randn(1, 3, 224, 224).astype(np.float32)\n\n# Prepare and run inference\nsession = AkidaInferenceSession(inference_model_path)\ninput_name = session.get_inputs()[0].name\noutputs = session.run(None, {input_name: input_samples})\nprint(f\"Output shape: {outputs[0].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary\n\nThe onnx2akida workflow enables you to:\n\n1. **Analyze** any ONNX model for Akida compatibility\n2. **Identify** which operations can be accelerated on Akida hardware\n3. **Generate** hybrid models that combine Akida acceleration with standard ONNX operators\n4. **Deploy** optimized inference models using ONNXRuntime\n\nThis approach maximizes hardware acceleration while maintaining full model functionality,\neven when only portions of the model are Akida-compatible.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}