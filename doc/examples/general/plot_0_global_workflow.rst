
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples/general/plot_0_global_workflow.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_examples_general_plot_0_global_workflow.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_general_plot_0_global_workflow.py:


Global MetaONNX workflow
=========================

This example demonstrates how to deploy an ONNX model to Akida hardware using
the onnx2akida toolkit. Starting from an ONNX model, we'll show how to:

1. Convert and analyze compatibility with Akida hardware
2. Display compatibility reports
3. Create hybrid models combining Akida-compatible and ONNX operators
4. Generate inference models for deployment

We'll use a MobileNetV4 model exported from HuggingFace as our example, though the workflow
applies to any ONNX model.

.. figure:: ../../img/execution_flow.png
   :target: ../../_images/execution_flow.png
   :alt: Overall MetaONNX workflow
   :scale: 25 %
   :align: center

   Global MetaONNX workflow

.. GENERATED FROM PYTHON SOURCE LINES 27-30

1. Export model to ONNX format
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 32-42

1.1. Export MobileNetV4 from HuggingFace
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We'll export a MobileNetV4 model from HuggingFace using the Optimum library.
This demonstrates the typical workflow of obtaining an ONNX model for analysis.

You can also export models from other frameworks:

* `tf2onnx <https://onnxruntime.ai/docs/tutorials/tf-get-started.html>`__ for TensorFlow
* `torch.onnx <https://docs.pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html>`__ for PyTorch

.. GENERATED FROM PYTHON SOURCE LINES 42-52

.. code-block:: Python


    import os

    from optimum.exporters.onnx import main_export

    model_dir = "mbv4"
    main_export("timm/mobilenetv4_conv_small.e2400_r224_in1k", output=model_dir)
    print(f"Model exported to {model_dir}/model.onnx")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Weight deduplication check in the ONNX export requires accelerate. Please install accelerate to run it.
    Model exported to mbv4/model.onnx




.. GENERATED FROM PYTHON SOURCE LINES 53-57

1.2. Load the ONNX model
^^^^^^^^^^^^^^^^^^^^^^^^^

Load the exported ONNX model for analysis.

.. GENERATED FROM PYTHON SOURCE LINES 57-65

.. code-block:: Python


    import onnx

    model_path = os.path.join(model_dir, "model.onnx")
    model = onnx.load(model_path)
    print(f"\nLoaded ONNX model from {model_path}")
    print(f"Model has {len(model.graph.node)} nodes.")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    Loaded ONNX model from mbv4/model.onnx
    Model has 89 nodes.




.. GENERATED FROM PYTHON SOURCE LINES 66-69

2. Convert to Akida
~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 71-80

2.1. Convert and get compatibility information
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The main entry point is the `convert <../../api_reference/onnx2akida_apis.html#onnx2akida.convert>`__
function, which analyzes the ONNX model and returns both a `HybridModel
<../../api_reference/onnx2akida_apis.html#hybridmodel>`__ and detailed compatibility information.
The `input_shape` parameter specifies the expected input dimensions for the model. Models can be
exported with a dynamic shape, but quantization and later Akida conversion and mapping need all
input dimensions to be fixed.

.. GENERATED FROM PYTHON SOURCE LINES 80-88

.. code-block:: Python


    from onnx2akida import convert

    # Convert the model and analyze compatibility
    # For MobileNetV4, the input shape is (channels, height, width)
    print("\nAnalyzing model compatibility with Akida hardware...")
    hybrid_model, compatibility_info = convert(model, input_shape=(3, 224, 224))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    Analyzing model compatibility with Akida hardware...
    Applied 1 of general pattern rewrite rules.
    Applied 1 of general pattern rewrite rules.
    Calibrating with 1/1.0 samples
    [INFO] Searching sequences... done
    Quantizing:   0%|          | 0/1 [00:00<?, ?it/s]    Quantizing: 100%|██████████| 1/1 [00:00<00:00,  3.21it/s]    Quantizing: 100%|██████████| 1/1 [00:00<00:00,  3.21it/s]
    /usr/local/lib/python3.11/dist-packages/cnn2snn/quantizeml/onnx_conversion/model_generator.py:68: UserWarning: Conversion stops  at node /blocks/blocks.3/blocks.3.0/dw_start/conv/Conv because of a dequantizer. The end of the graph is ignored:
     ___________________________________________________
    Node (type)
    ===================================================
    /blocks/blocks.3/blocks.3.0/dw_start/conv/Conv (QuantizedDepthwise2DBiasedScaled)
    ===================================================
    . 
     This can be expected for model heads (e.g. softmax for classification) but could also mean that processing layers were not quantized.
      warnings.warn(f"Conversion stops {stop_layer_msg} because of a dequantizer. "
    /usr/local/lib/python3.11/dist-packages/cnn2snn/quantizeml/onnx_conversion/model_generator.py:68: UserWarning: Conversion stops  at node /blocks/blocks.4/blocks.4.0/conv/Conv because of a dequantizer. The end of the graph is ignored:
     ___________________________________________________
    Node (type)
    ===================================================
    /blocks/blocks.4/blocks.4.0/conv/Conv (QuantizedConv2DBiasedGlobalAvgPoolReLUScaled)
    ===================================================
    . 
     This can be expected for model heads (e.g. softmax for classification) but could also mean that processing layers were not quantized.
      warnings.warn(f"Conversion stops {stop_layer_msg} because of a dequantizer. "
    /usr/local/lib/python3.11/dist-packages/cnn2snn/quantizeml/onnx_conversion/model_generator.py:68: UserWarning: Conversion stops  at node /classifier/Gemm because of a dequantizer. The end of the graph is ignored:
     ___________________________________________________
    Node (type)
    ===================================================
    /classifier/Gemm (QuantizedDense1DFlattenBiased)
    ===================================================
    . 
     This can be expected for model heads (e.g. softmax for classification) but could also mean that processing layers were not quantized.
      warnings.warn(f"Conversion stops {stop_layer_msg} because of a dequantizer. "
    Converting:   0%|          | 0/3 [00:00<?, ?it/s]    Converting:  67%|██████▋   | 2/3 [00:00<00:00,  5.89it/s]    Converting: 100%|██████████| 3/3 [00:00<00:00,  8.82it/s]




.. GENERATED FROM PYTHON SOURCE LINES 89-97

2.2. Display compatibility report
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The obtained `ModelCompatibilityInfo
<../../api_reference/onnx2akida_apis.html#onnx2akida.compatibility_info.ModelCompatibilityInfo>`__
object contains detailed information about which nodes and subgraphs are compatible with Akida
hardware. Use `print_report <../../api_reference/onnx2akida_apis.html#onnx2akida.print_report>`__
to display a comprehensive analysis.

.. GENERATED FROM PYTHON SOURCE LINES 97-103

.. code-block:: Python


    from onnx2akida import print_report

    # Print detailed compatibility report
    print_report(compatibility_info, hybrid_model)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [INFO]: Percentage of nodes compatible with akida: 100.0000 %

    List of backends exchanges:
     • CPU to Akida at layer /conv_stem/Conv: 147.000 KB
     • Akida to CPU at layer /classifier/Gemm: 3.906 KB





.. GENERATED FROM PYTHON SOURCE LINES 104-111

The report shows:

- The list of incompatibles operation types,
- The list of incompatibilities indexed by node and by stage (quantization, conversion, mapping)
  indicating where an incompatibility was found and why,
- Overall compatibility percentage,
- The memory report for Akida to CPU transfers.

.. GENERATED FROM PYTHON SOURCE LINES 113-127

2.3. Understanding the HybridModel
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The returned `HybridModel <../../api_reference/onnx2akida_apis.html#hybridmodel>`__ object
represents a model that can contain both:

* Akida-compatible submodels (will be accelerated on Akida hardware)
* Standard ONNX operators (will run on CPU via ONNXRuntime)

This hybrid approach allows partial acceleration even when not all operations
are Akida-compatible.

.. Warning:: Inference is not possible on the `HybridModel` directly. You have to explicitly
             generate an inference model as shown in the next section.

.. GENERATED FROM PYTHON SOURCE LINES 129-132

3. Generate inference model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 134-142

3.1. Generate hybrid inference model with Akida device
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To create a deployable inference model, you need an Akida device.

.. important::
   A 2.0 FPGA device like available in `Akida Cloud <https://brainchip.com/aclp/>`__ is used here
   for demonstration.

.. GENERATED FROM PYTHON SOURCE LINES 142-149

.. code-block:: Python


    import akida

    # Check for available Akida devices
    assert len(devices := akida.devices()) > 0, "No device found, this example needs a 2.0 device."
    print(f'Available devices: {[dev.desc for dev in devices]}')





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Available devices: ['fpga-1766']




.. GENERATED FROM PYTHON SOURCE LINES 150-154

Inference happens on a device, so we need to map the hybrid model onto it. This can be done using
`HybridModel.map
<../../api_reference/onnx2akida_apis.html#onnx2akida.hybrid_model.HybridModel.map>`__ like shown
below.

.. GENERATED FROM PYTHON SOURCE LINES 154-162

.. code-block:: Python


    # Map on the device
    fpga_device = devices[0]
    try:
        hybrid_model.map(fpga_device)
    except RuntimeError as e:
        print("Mapping failed:\n", e)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Mapping failed:
     Failed to map Akida model at index 0 within 'akida_models'. Reason: Cannot map layer '/conv_head/Conv'. Not enough hardware components of type CNP1 available. 27 are needed but 21 are available..




.. GENERATED FROM PYTHON SOURCE LINES 163-168

Mapping the HybridModel onto the Akida device after conversion might fail: while some layers are
supported by Akida hardware, they might not fit on device due to resource constraints.
In such cases, you can try mapping on a larger virtual device - but that cannot be used for
inference, it only serves for prototyping - or you can go back to model conversion and provide the
device as a `convert <../../api_reference/onnx2akida_apis.html#onnx2akida.convert>`__ parameter.

.. GENERATED FROM PYTHON SOURCE LINES 168-171

.. code-block:: Python


    hybrid_model, compatibility_info = convert(model, input_shape=(3, 224, 224), device=fpga_device)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Applied 1 of general pattern rewrite rules.
    Applied 1 of general pattern rewrite rules.
    Calibrating with 1/1.0 samples
    [INFO] Searching sequences... done
    Quantizing:   0%|          | 0/1 [00:00<?, ?it/s]    Quantizing: 100%|██████████| 1/1 [00:00<00:00,  3.09it/s]    Quantizing: 100%|██████████| 1/1 [00:00<00:00,  3.09it/s]
    /usr/local/lib/python3.11/dist-packages/cnn2snn/quantizeml/onnx_conversion/model_generator.py:68: UserWarning: Conversion stops  at node /blocks/blocks.3/blocks.3.0/dw_start/conv/Conv because of a dequantizer. The end of the graph is ignored:
     ___________________________________________________
    Node (type)
    ===================================================
    /blocks/blocks.3/blocks.3.0/dw_start/conv/Conv (QuantizedDepthwise2DBiasedScaled)
    ===================================================
    . 
     This can be expected for model heads (e.g. softmax for classification) but could also mean that processing layers were not quantized.
      warnings.warn(f"Conversion stops {stop_layer_msg} because of a dequantizer. "
    /usr/local/lib/python3.11/dist-packages/cnn2snn/quantizeml/onnx_conversion/model_generator.py:68: UserWarning: Conversion stops  at node /blocks/blocks.4/blocks.4.0/conv/Conv because of a dequantizer. The end of the graph is ignored:
     ___________________________________________________
    Node (type)
    ===================================================
    /blocks/blocks.4/blocks.4.0/conv/Conv (QuantizedConv2DBiasedGlobalAvgPoolReLUScaled)
    ===================================================
    . 
     This can be expected for model heads (e.g. softmax for classification) but could also mean that processing layers were not quantized.
      warnings.warn(f"Conversion stops {stop_layer_msg} because of a dequantizer. "
    /usr/local/lib/python3.11/dist-packages/cnn2snn/quantizeml/onnx_conversion/model_generator.py:68: UserWarning: Conversion stops  at node /classifier/Gemm because of a dequantizer. The end of the graph is ignored:
     ___________________________________________________
    Node (type)
    ===================================================
    /classifier/Gemm (QuantizedDense1DFlattenBiased)
    ===================================================
    . 
     This can be expected for model heads (e.g. softmax for classification) but could also mean that processing layers were not quantized.
      warnings.warn(f"Conversion stops {stop_layer_msg} because of a dequantizer. "
    /usr/local/lib/python3.11/dist-packages/cnn2snn/quantizeml/onnx_conversion/model_generator.py:68: UserWarning: Conversion stops  at node node_Conv_1 because of a dequantizer. The end of the graph is ignored:
     ___________________________________________________
    Node (type)
    ===================================================
    node_Conv_1 (QuantizedDepthwise2DReLUScaled)
    ===================================================
    . 
     This can be expected for model heads (e.g. softmax for classification) but could also mean that processing layers were not quantized.
      warnings.warn(f"Conversion stops {stop_layer_msg} because of a dequantizer. "
    /usr/local/lib/python3.11/dist-packages/cnn2snn/quantizeml/onnx_conversion/model_generator.py:68: UserWarning: Conversion stops  at node /blocks/blocks.3/blocks.3.0/dw_mid/conv/Conv because of a dequantizer. The end of the graph is ignored:
     ___________________________________________________
    Node (type)
    ===================================================
    /blocks/blocks.3/blocks.3.0/dw_mid/conv/Conv (QuantizedDepthwise2DBiasedReLUScaled)
    ===================================================
    . 
     This can be expected for model heads (e.g. softmax for classification) but could also mean that processing layers were not quantized.
      warnings.warn(f"Conversion stops {stop_layer_msg} because of a dequantizer. "
    Converting:   0%|          | 0/7 [00:00<?, ?it/s]    Converting:  71%|███████▏  | 5/7 [00:00<00:00, 27.65it/s]/usr/local/lib/python3.11/dist-packages/cnn2snn/quantizeml/onnx_conversion/model_generator.py:68: UserWarning: Conversion stops  at node /conv_head/Conv because of a dequantizer. The end of the graph is ignored:
     ___________________________________________________
    Node (type)
    ===================================================
    /conv_head/Conv (QuantizedConv2DBiasedReLUScaled)
    ===================================================
    . 
     This can be expected for model heads (e.g. softmax for classification) but could also mean that processing layers were not quantized.
      warnings.warn(f"Conversion stops {stop_layer_msg} because of a dequantizer. "
    Converting: 100%|██████████| 7/7 [00:00<00:00, 13.05it/s]




.. GENERATED FROM PYTHON SOURCE LINES 172-174

.. code-block:: Python

    print_report(compatibility_info, hybrid_model)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    Set of Incompatible op_types: ['Conv', 'Relu']
    List of Incompatibilities:
     ❌ Node sequence: [/conv_head/Conv(op_type=Conv), /norm_head/act/Relu(op_type=Relu)]
         • Stage: Mapping
         • Faulty node: /conv_head/Conv
         • Reason: Cannot map layer '/conv_head/Conv'. Not enough hardware components of type CNP1 available. 27 are needed but 24 are available.

    [INFO]: Percentage of nodes compatible with akida: 97.8261 %

    List of backends exchanges:
     • CPU to Akida at layer /conv_stem/Conv: 147.000 KB
     • Akida to CPU at layer node_Conv_1: 36.750 KB
     • CPU to Akida at layer /blocks/blocks.2/blocks.2.0/pw_proj/conv/Conv: 36.750 KB
     • Akida to CPU at layer /blocks/blocks.3/blocks.3.0/dw_start/conv/Conv: 18.375 KB
     • CPU to Akida at layer /blocks/blocks.3/blocks.3.0/pw_exp/conv/Conv: 18.375 KB
     • Akida to CPU at layer /blocks/blocks.3/blocks.3.0/dw_mid/conv/Conv: 27.562 KB
     • CPU to Akida at layer /blocks/blocks.3/blocks.3.0/pw_proj/conv/Conv: 27.562 KB
     • Akida to CPU at layer /blocks/blocks.4/blocks.4.0/conv/Conv: 0.938 KB
     • CPU to Akida at layer /classifier/Gemm: 1.250 KB
     • Akida to CPU at layer /classifier/Gemm: 3.906 KB





.. GENERATED FROM PYTHON SOURCE LINES 175-179

The conversion algorithm knows the resource limitations, so it now avoids converting parts
that do not fit on the device. That is why there are more incompatibilities (the node that was too
big to fit on 6-node device will run on CPU), but operations that were mapped on the device can be
accelerated by it.

.. GENERATED FROM PYTHON SOURCE LINES 179-183

.. code-block:: Python


    # Generate the inference model
    infer_model = hybrid_model.generate_inference_model()








.. GENERATED FROM PYTHON SOURCE LINES 184-188

3.2. Save the inference model
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Once generated, the inference model can be saved for deployment.

.. GENERATED FROM PYTHON SOURCE LINES 188-192

.. code-block:: Python


    inference_model_path = "model_inference.onnx"
    onnx.save(infer_model, inference_model_path)








.. GENERATED FROM PYTHON SOURCE LINES 193-198

The inference model is a standard ONNX model that can be executed using ONNXRuntime. It's graph
can be visualized with `Netron <https://netron.app>`__ and it will show ``AkidaOp`` nodes that are
custom wrappers for all Akida-accelerated submodels. It will also contain ``Transpose`` nodes
between ONNX and AkidaOp operators are automatically inserted to handle the different data layout
conventions (NCHW for ONNX, NHWC for Akida).

.. GENERATED FROM PYTHON SOURCE LINES 200-205

3.3. Perform an inference
^^^^^^^^^^^^^^^^^^^^^^^^^

The inference model can be executed using ONNXRuntime and the provided `AkidaInferenceSession
<../../api_reference/onnx2akida_apis.html#onnx2akida.inference.AkidaInferenceSession>`__.

.. GENERATED FROM PYTHON SOURCE LINES 205-218

.. code-block:: Python


    import numpy as np
    from onnx2akida.inference import AkidaInferenceSession

    # Generate random input samples with shape (batch_size, channels, height, width)
    input_samples = np.random.randn(1, 3, 224, 224).astype(np.float32)

    # Prepare and run inference
    session = AkidaInferenceSession(inference_model_path)
    input_name = session.get_inputs()[0].name
    outputs = session.run(None, {input_name: input_samples})
    print(f"Output shape: {outputs[0].shape}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Output shape: (1, 1000)




.. GENERATED FROM PYTHON SOURCE LINES 219-231

4. Summary
~~~~~~~~~~

The onnx2akida workflow enables you to:

1. **Analyze** any ONNX model for Akida compatibility
2. **Identify** which operations can be accelerated on Akida hardware
3. **Generate** hybrid models that combine Akida acceleration with standard ONNX operators
4. **Deploy** optimized inference models using ONNXRuntime

This approach maximizes hardware acceleration while maintaining full model functionality,
even when only portions of the model are Akida-compatible.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 22.477 seconds)


.. _sphx_glr_download_examples_general_plot_0_global_workflow.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_0_global_workflow.ipynb <plot_0_global_workflow.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_0_global_workflow.py <plot_0_global_workflow.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_0_global_workflow.zip <plot_0_global_workflow.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
